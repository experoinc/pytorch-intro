{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "## Intro to Tensors, Variables, Gradients and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports to get started\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Tensor?\n",
    "\n",
    "A Tensor, in simple terms, is an N dimensional array.\n",
    "\n",
    "In torch, we must use tensors to express every aspect of our data - input features, hidden layers, weights, and output all get efficiently computed using tensors.\n",
    "\n",
    "Lets look at a few basic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-37 *\n",
       "  7.4749  0.0005 -1.8040\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a couple of tensors.\n",
    "\n",
    "Note one that Tensor() actually creates a FloatTensor.  Other types that could be useful are:\n",
    "- LongTensor (for integers)\n",
    "- DoubleTensor\n",
    "- HalfTensor (so hot right now).\n",
    "\n",
    "Note also that our new tensors are not initialized at all.  Lets instead use rand for a uniform value between [0,1] or randn for a random normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.0139  0.4146  0.3610  0.3006  0.0042\n",
       " [torch.FloatTensor of size 1x5], \n",
       "  0.0068 -0.1561  0.4589 -0.3152  2.3001\n",
       " [torch.FloatTensor of size 1x5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,5)\n",
    "y = torch.randn(1,5)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once in tensor form, torch provides many utilities to play with, such as sum(), mean().\n",
    "\n",
    "As a rule of thumb, methods on a tensor with \"_\" post-fixed are performed in-place on the tensor and will alter the tensor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5.0139  5.4146  5.3611  5.3006  5.0042\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n",
      "\n",
      " 0.0139  0.4146  0.3610  0.3006  0.0042\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n",
      "\n",
      " 5.0139  5.4146  5.3611  5.3006  5.0042\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n",
      "\n",
      " 5.0139  5.4146  5.3611  5.3006  5.0042\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0139  0.4146  0.3611  0.3006  0.0042\n",
       "[torch.FloatTensor of size 1x5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.add(5)) # x is not altered\n",
    "print(x)\n",
    "\n",
    "print(x.add_(5)) # x is altered\n",
    "print(x)\n",
    "\n",
    "x.sub_(5) #reset back to where it was"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables, Gradients and the Compute History\n",
    "\n",
    "At a high level, machine learning optimizes a function to reduce loss.  For this, we need gradients of our variables.  This goes back to [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method) of finding roots: Compute the derivative, compute f'(x) = 0 for x, repeat with new x.\n",
    "\n",
    "![newts](https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif)\n",
    "\n",
    "When we wrap our Tensors in a Variable, a couple of cool things happen:\n",
    "\n",
    "- The computation history of the variable is stored\n",
    "- The gradient of the value can be computed\n",
    "\n",
    "Lets take a look at what that means for a simple example:\n",
    "\n",
    "Lets say we have a function y = x^2 + 0.5\n",
    "\n",
    "dy/dx = 2x.\n",
    "\n",
    "so at x = 1.0, we expect dy/dx = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([1.0]), requires_grad=True)\n",
    "y = x * x + 0.5\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you catch all that?\n",
    "- We created a variable for x, initializing it with the value 1.0\n",
    "- Then we defined y as x^2 + 0.5\n",
    "- Then we called backward() to get the gradient, which, as expected\n",
    "- The gradient value at x=1.0 for y=x^2 + 0.5 was 2.0\n",
    "\n",
    "Also, lets not ignore the *history* of y, going in reverse order, add 0.5, multiply x by x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x7f54d01d2dd8>, ((<MulBackward1 at 0x7f54d01d2e48>, 0),))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn, y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this history also describes the operations to be performed on the graph.\n",
    "in fact, when we say y = x * x + 0.5, y does not immediately get evaluated to 1.5.  This evaluation comes later, when we actually request to see the value of y.\n",
    "\n",
    "This compute graph is a form of \"lazy evaluation\" - its important because our inputs -will- be massive, our computations complex.  We don't want our computer to hang every time we twiddle with our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "OK, we've seen the basics, lets apply these simple core principles to a simple minimization problem:\n",
    "\n",
    "for\n",
    "\n",
    "y = (x + 0.2)^2 +4.0\n",
    "\n",
    "find x to minimize y\n",
    "\n",
    "\n",
    "Of course, the answer is -0.2, but lets see it in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   0:  0.76  5.44\n",
      "step   1:  0.57  4.92\n",
      "step   2:  0.41  4.59\n",
      "step   3:  0.29  4.38\n",
      "step   4:  0.19  4.24\n",
      "step   5:  0.11  4.15\n",
      "step   6:  0.05  4.10\n",
      "step   7:  0.00  4.06\n",
      "step   8: -0.04  4.04\n",
      "step   9: -0.07  4.03\n",
      "step  10: -0.10  4.02\n",
      "step  11: -0.12  4.01\n",
      "step  12: -0.13  4.01\n",
      "step  13: -0.15  4.00\n",
      "step  14: -0.16  4.00\n",
      "step  15: -0.17  4.00\n",
      "step  16: -0.17  4.00\n",
      "step  17: -0.18  4.00\n",
      "step  18: -0.18  4.00\n",
      "step  19: -0.19  4.00\n",
      "step  20: -0.19  4.00\n",
      "step  21: -0.19  4.00\n",
      "step  22: -0.19  4.00\n",
      "step  23: -0.19  4.00\n",
      "step  24: -0.20  4.00\n",
      "step  25: -0.20  4.00\n",
      "Converged on solution -> x = -0.20 in 25 iterations\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([1.0]), requires_grad=True)\n",
    "rate = 0.1\n",
    "\n",
    "for i in range(100):\n",
    "    y = (x + 0.2) * (x + 0.2) + 4.0\n",
    "    y.backward()\n",
    "    delta = rate * x.grad.data\n",
    "    x.data -= delta\n",
    "    x.grad.data.zero_()\n",
    "\n",
    "    print(\"step {:3}: {:5.2f} {:5.2f}\".format(i, x.data[0], y.data[0]))\n",
    "    if abs(delta[0]) < 0.001:\n",
    "        break\n",
    "\n",
    "print(\"Converged on solution -> x = {:5.2f} in {} iterations\".format(x.data[0], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was that?\n",
    "Here we implimented [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) to minimize our objective function.  Easy right?\n",
    "\n",
    "A few key notes here.\n",
    "9. We use \"rate\" factor to make the minimization a little less aggressive.\n",
    "9. We must zero our gradients between iterations - otherwise they keep increasing\n",
    "\n",
    "Lets do the same thing, except this time, lets use a built-in pytorch optimizer for SGD.  Actually, lets make everything look more like a proper machine learning problem now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   0:  0.76  4.00\n",
      "step   1:  0.57  4.00\n",
      "step   2:  0.41  4.00\n",
      "step   3:  0.29  4.00\n",
      "step   4:  0.19  4.00\n",
      "step   5:  0.11  4.00\n",
      "step   6:  0.05  4.00\n",
      "step   7:  0.00  4.00\n",
      "step   8: -0.04  4.00\n",
      "step   9: -0.07  4.00\n",
      "step  10: -0.10  4.00\n",
      "step  11: -0.12  4.00\n",
      "step  12: -0.13  4.00\n",
      "step  13: -0.15  4.00\n",
      "step  14: -0.16  4.00\n",
      "step  15: -0.17  4.00\n",
      "step  16: -0.17  4.00\n",
      "step  17: -0.18  4.00\n",
      "step  18: -0.18  4.00\n",
      "step  19: -0.19  4.00\n",
      "step  20: -0.19  4.00\n",
      "step  21: -0.19  4.00\n",
      "step  22: -0.19  4.00\n",
      "step  23: -0.19  4.00\n",
      "step  24: -0.20  4.00\n",
      "step  25: -0.20  4.00\n",
      "Converged on solution -> x = -0.20 in 25 iterations\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "x = Variable(torch.FloatTensor([1.0]), requires_grad=True)\n",
    "learning_rate = rate\n",
    "\n",
    "optimizer = SGD([x], lr = learning_rate)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = (x + 0.2) * (x + 0.2) + 4.0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"step {:3}: {:5.2f} {:5.2f}\".format(i, x.data[0], y.data[0]))\n",
    "    if (abs(learning_rate * x.grad.data[0]) < 0.001):\n",
    "        break\n",
    "        \n",
    "print(\"Converged on solution -> x = {:5.2f} in {} iterations\".format(x.data[0], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look familiar?  It should!\n",
    "\n",
    "### Loss\n",
    "We've replace \"y\" with \"loss\" to begin speaking the language of Machine Learning.\n",
    "\n",
    "### Learning Rate\n",
    "We've also gotten a look at our first \"hyper-parameter\": The learning rate.  We must be careful with the learning rate.  Note in the above example, if our learning rate is 1.0, we will ossilate between two values of x forever, never converging.  If we set it too low, it should converge, but it will take a long time indeed.\n",
    "\n",
    "Usually, its better to use a smaller learning rate, then adjust upwards to find a balance between quality of result and convergence time.  IF your loss is oscillating around a value, your learning rate is likely too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
