{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "# Intro to Neural Networks\n",
    "Lets use some simple models and try to match some simple problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.models as models\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Before we dive deep into the nerual net, lets take a brief aside to discuss data loading.\n",
    "\n",
    "Pytorch provides a Dataset class which is fairly easy to inherit from.  We need only implement two methods for our data load:\n",
    "9. __len__(self) -> return the size of our dataset\n",
    "9. __getitem__(self, idx) -> return a data at a given index.\n",
    "\n",
    "The *real* benefit of implimenting a Dataset class comes from using the DataLoader class.\n",
    "For data sets which are too large to fit into memory (or more likely, GPU memory), the DataLoader class gives us two advantages:\n",
    "9. Efficient shuffling and random sampling for batches\n",
    "9. Data is loaded in a seperate *processes*.\n",
    "\n",
    "Number (2) above is *important*.  The Python interpretter is single threaded only, enforced with a GIL (Global Interpreter Lock).  Without (2), we waste valuable (and potentially expensive) processing time shuffling and sampling and building tensors.  \n",
    "So lets invest a little time to build a Dataset and use the DataLoader.\n",
    "\n",
    "In or example below, we are going to mock a dataset with a simple function, this time:\n",
    "\n",
    "y = sin(x) + 0.01 * x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impliment the dataset\n",
    "class FunctionalDataset(Dataset):\n",
    "    def __init__(self, fun=None):\n",
    "        super(FunctionalDataset, self).__init__()\n",
    "        if fun:\n",
    "            self.fun = fun\n",
    "        else:\n",
    "            self.fun = lambda x: np.sin(2. * x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 201\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = 0.01 * idx - 1.0\n",
    "        return np.float32(x), np.float32(self.fun(x))\n",
    "\n",
    "#create a data loader with a subprocess for feeding data\n",
    "dataset = FunctionalDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=101, shuffle=True, num_workers=1)\n",
    "\n",
    "#lets sample a few batches and plot the results to see our function\n",
    "for i, (X, Y) in enumerate(dataloader):\n",
    "    plt.scatter(X,Y, label='Batch: {}'.format(i))\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First Neural Net\n",
    "Lets now build our first neural net.\n",
    "\n",
    "In this case, we'll take a classic approach with 2 fully connected hidden layers and a fully connected output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FirstNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "net = FirstNet(input_size=1, hidden_size=64, num_classes=1)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a few key features of our net:\n",
    "\n",
    "1) We have 3 fully connected layers, defined in our init function.\n",
    "\n",
    "2) We define a *forward pass* method which is the prediction of the neural net given an input X\n",
    "\n",
    "3) Note that we make a *view* of our input array.  In our simple model, we expect a 1D X value, and we output a 1D Y value.  For efficiency, we may wish to pass in *many* X values, particularly when training.  Thus, we need to set up a *view* of our input array: Many 1D X values.  -1 in this case indicates that the first dimension (number of X values) is inferred from the tensor's shape.\n",
    "\n",
    "### Logging and Visualizing to TensorboardX\n",
    "\n",
    "Lets track the progress of our training and visualize in tensorboard (using tensorboardX).  We'll also add a few other useful functions to help visualize things.\n",
    "\n",
    "To view the output, run:\n",
    "`tensorboard --logdir nb/run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbwriter = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualization and Batching\n",
    "We will begin by adding a graph visualization to tensorboard.  To do this, we need a valid input to our network.\n",
    "\n",
    "Our network is simple - floating point in, floating point out.  *However*, pytorch expects us to *batch* our inputs - therefore it expects an *array* of inputs instead of a single input.  There are many ways to work around this, I like \"unsqueeze\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.FloatTensor([0.0]))\n",
    "tbwriter.add_graph(net, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda\n",
    "IF you have a GPU available, your training will run much faster.\n",
    "Moving data back and forth between the CPU and the GPU is fairly straightforward - although it can be easy to forget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFig(iteration):\n",
    "    X = np.linspace(-1, 1, 201, dtype=np.float32)\n",
    "    X = torch.FloatTensor(X)\n",
    "    X = Variable(X)\n",
    "    if use_cuda:\n",
    "        Y = net.forward(X.cuda()).cpu()\n",
    "    else:\n",
    "        Y = net.forward(X)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(X.data.numpy(), Y.data.numpy())\n",
    "    plt.title('Prediciton at iter: {}'.format(iteration))\n",
    "    return fig\n",
    "    \n",
    "def showFig(iteration):\n",
    "    fig = makeFig(iteration)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def logFig(iteration):\n",
    "    fig = makeFig(iteration)\n",
    "    fig.canvas.draw()\n",
    "    raw = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    raw = raw.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    tbwriter.add_image('Prediction at iter: {}'.format(iteration), raw)\n",
    "    plt.close()\n",
    "    \n",
    "showFig(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have a ways to go.  Lets use our data loader and do some training.  Here we will use MSE loss (mean squared error) and SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 2000\n",
    "\n",
    "if use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, Y in dataloader:\n",
    "        if use_cuda:\n",
    "            X = Variable(X).cuda()\n",
    "            Y = Variable(Y).cuda()\n",
    "        else:\n",
    "            X = Variable(X)\n",
    "            Y = Variable(Y)\n",
    "        pred = net.forward(X)\n",
    "        loss = criterion(pred, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tbwriter.add_scalar(\"Loss\", loss.data[0])\n",
    "\n",
    "    if (epoch % 100 == 99):\n",
    "        print(\"Epoch: {:>4} Loss: {}\".format(epoch, loss.data[0]))\n",
    "        for name, param in net.named_parameters():\n",
    "            tbwriter.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "        logFig(epoch)\n",
    "        \n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showFig(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We've written our first network, take a moment and play with some of our models here.\n",
    "\n",
    "Try inputting a different function into the functional dataset, such as:\n",
    "  dataset = FunctionalDataset(lambda x: 1.0 if x > 0 else -1.0\n",
    "\n",
    "Try experimenting with the network - change the number of neurons in the layer, or add more layers.\n",
    "  \n",
    "Try changing the learning rate (and probably the number of epochs).\n",
    "\n",
    "And lastly, try disabling cuda (if you have a gpu).\n",
    "\n",
    "#### How well does the prediction match our input function?\n",
    "#### How long does it take to train?\n",
    "\n",
    "One last note: we are absolutely *over-fitting* our dataset here.  In this example, that's ok.  For real work, we will need to be more careful.\n",
    "\n",
    "Speaking of real work, lets do some real work identifying customer cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
